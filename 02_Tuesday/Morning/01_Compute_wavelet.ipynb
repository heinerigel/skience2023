{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\")) \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "from msnoise.api import *\n",
    "from wxs_dvv import *\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "plt.rcParams['figure.figsize'] = (16,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19d48e3-a573-4177-95c3-8ba3f7f3effc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compute dv/v using the wavelet method\n",
    "\n",
    "Following Shujuan's presentation we use a pre-processed dataset of cross-correlations functions (CCFs) to exeriment with the wavelet method.\n",
    "Here, we will play with single station (SC) products of station \"CI.LJR\" in the Tejon Pass, between the San Emig-\n",
    "dio and Tehachapi Mountains, California.\n",
    "The station was selected as it was used in [Clements and Denolle (2023, JGR)](https://doi.org/10.1093/gji/ggz495) who used it compare the dv/v with variations of groundwater levels in Southern California . We look at the 2011 to 2017 period, as it includes the 2011-2016 severe drought period that significantly impacted the ground water level. A fitting subject for thhe theme of this workshop.\n",
    "\n",
    "![Figure 3 of Clements and Denolle (2023)](Figures/Clement2023.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339c32c-f2be-4ce9-906b-1ed8b2015a93",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Here we will try and reproduce the dv/v curve for that station, first using similar parameters (Frequency range, time lags), and then individually explore other parametrizations. Here we are using the wavelet method, meaning that we can easily adjust the frequeny range of interest after processing the CCFs\n",
    "\n",
    "- Because the authors made a very good effort at making their work reproducible (Yay!) we can easily compare the resulting time series to theirs.\n",
    "\n",
    "- Depending on the horse-power under the hood of your laptop, the processing of the full period (2011-2017) can take a while. Adjust the processing to your taste (e.i. time period or components configuration). Use this processing time efficiently by taking a look at the CCFs through MSNoise, either using the command line or a new/seperate Jupyter notebook (Keep an eye on yesterday's notebook!).\n",
    "\n",
    "- If you quickly go through the full practical, go back to yesterday afternoon's dataset and play with wavelet method to measure the dv/v in different filters with a high spectral resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8467f3-be76-41b0-be68-114a80fc6b4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting parameters from the msnoise DB\n",
    "\n",
    "When pluging into an MSNoise workflow, you can always use the parameters as they are in the database, or modify them below to explore the results (e.i. Start, end freqmin, freqmax, mov_stack,...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = connect()\n",
    "params = get_params(db)\n",
    "filterid=1 #Make sure this is the proper frequency band\n",
    "fs = params.cc_sampling_rate\n",
    "lag_min = params.dtt_minlag\n",
    "lag_max = params.dtt_minlag+params.dtt_width\n",
    "\n",
    "# We'll start with a broad-ish frequency range, and play with it later\n",
    "freqmin=0.1\n",
    "freqmax=4.0\n",
    "\n",
    "# Ask me about this variable if your curious!\n",
    "subdaily=False\n",
    "\n",
    "saveit=True\n",
    "plot = False\n",
    "\n",
    "\n",
    "stations_to_analyse = [\"%s.%s.%s\" % (sta.net, sta.sta, sta.locs()[0]) for sta in get_stations(db, all=False)]\n",
    "pairs = [\"{}:{}\".format(sta,sta) for sta in stations_to_analyse]\n",
    "\n",
    "start = params.startdate\n",
    "end = params.enddate\n",
    "\n",
    "comps = [\"EZ\", \"NZ\", \"EN\"]\n",
    "mov_stack = 5\n",
    "\n",
    "# Obtain a list of dates between ``start_date`` and ``enddate``\n",
    "_, _, datelist = build_movstack_datelist(db)\n",
    "taxis = get_t_axis(db)\n",
    "mov_stack = 1\n",
    "\n",
    "# Get the results for two station, filter id=1, ZZ component, mov_stack=1 and the results as a 2D array:\n",
    "n, ccfs = get_results(db, pairs[0].split(\":\")[0], pairs[0].split(\":\")[0], filterid, \"EZ\", datelist,\n",
    "                      1, format=\"matrix\", params=params)\n",
    "# Convert to a pandas DataFrame object for convenience, and drop empty rows:\n",
    "df = pd.DataFrame(ccfs, index=pd.DatetimeIndex(datelist), columns=taxis)\n",
    "df = df.dropna()\n",
    "\n",
    "# Define the 99% percentile of the data, for visualisation purposes:\n",
    "clim = df.mean(axis=\"index\").quantile(0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d79f5d-4041-4617-87cb-98ce72abbb71",
   "metadata": {},
   "source": [
    "## What have we got?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c579eb-acbf-48ff-ad28-3a3b94d2544e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.pcolormesh(df.columns, df.index.to_pydatetime(), df.values,\n",
    "               vmin=-clim, vmax=clim, rasterized=True, cmap=\"RdBu\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Interferogram\")\n",
    "plt.xlabel(\"Lag Time (s)\")\n",
    "plt.ylim(df.index[0],df.index[-1])\n",
    "plt.xlim(-20, 20)\n",
    "plt.subplots_adjust(left=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc585ed-896f-496c-b6ea-16d4c5300e5d",
   "metadata": {},
   "source": [
    "# Starting simple\n",
    "\n",
    "- First we are only looking at one trace and the reference.\n",
    "\n",
    "- You were only given the daily CCFs, so you will need to stack them before they can be used.\n",
    "\n",
    "- This is the occasion for you to add other mov_stacks to the configuration if you want (What about 90 days?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87589e6d-23d2-487f-b499-f64d7ecd8668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! msnoise reset STACK -a\n",
    "! msnoise cc stack -r\n",
    "! msnoise reset STACK\n",
    "! msnoise cc stack -m\n",
    "! msnoise info -j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5170b8-ccf0-40a2-92ce-03a7fd4ab6c8",
   "metadata": {},
   "source": [
    "Let's look at one day first\n",
    "\n",
    "Here we are reading the CCF files for the reference  and the first day analysed\n",
    "we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.date_range(start, end,freq=\"D\")[0]# chose a day to plot, this just picks the first one but play with others!\n",
    "pair = pairs[0]\n",
    "sta1 = pair.split(\":\")[0]\n",
    "sta2 = sta1\n",
    "comp = \"NZ\"\n",
    "ref_file = \"STACKS/{:02d}/REF/{}/{}_{}.MSEED\".format(filterid,\n",
    "                                                  comp,\n",
    "                                                  sta1,\n",
    "                                                  sta2)\n",
    "\n",
    "fn2 = \"STACKS/{:02d}/{:03d}_DAYS/{}/{}_{}/{}.MSEED\".format(filterid,mov_stack,comp,\n",
    "                                                   sta1,\n",
    "                                                   sta2,\n",
    "                                                   date.date())\n",
    "ref=read(ref_file)[0].data\n",
    "current = read(fn2)[0].data\n",
    "t = read(fn2)[0].times()-120\n",
    "ori_waveform = (ref/ref.max())\n",
    "new_waveform = (current/current.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852282be-8624-4879-95b3-23ae8a00e022",
   "metadata": {},
   "source": [
    "Let's take a look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1897324-26fb-4f2c-b3d8-227e6ad5ea3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(t,ori_waveform, label=\"Current trace\")\n",
    "plt.plot(t,new_waveform, label=\"Reference\", alpha=0.7)\n",
    "plt.xlim(-50,50)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd2c8f6-7872-4a29-a698-a5e9c1bef9d5",
   "metadata": {},
   "source": [
    "## The cross-wavelet transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a412640f-f10e-415a-92c7-a38beb35c8e3",
   "metadata": {},
   "source": [
    "Now we use the same function described in Shujuan's paper on both traces\n",
    "\n",
    "The inputs are the following:\n",
    "\n",
    "    trace_ref,\n",
    "    trace_current,\n",
    "    fs, Sampling frequency --> extracted from the DB\n",
    "    ns, smoothing parameter\n",
    "    nt, smoothing parameter\n",
    "    vpo, Spacing parameter between discrete scales, higher means finer resolution\n",
    "    freqmin,\n",
    "    freqmax,\n",
    "    nptsfreq, Number of frequency points between freqmin and freqmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26027e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross wavelet transform\n",
    "WXamp, WXspec, WXangle, Wcoh, WXdt, freqs, coi = xwt(ori_waveform, new_waveform, fs, 3, 0.25, 10, freqmin, freqmax, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9145a436-81e7-47c0-b946-ca7eaad4588f",
   "metadata": {},
   "source": [
    "While most of the magic happened in the cell above, we still don't have a dv/v.\n",
    "For this we will calculate a similar linear regression as the one discussed yesterday for the MWCS method. Here, however, we calculate it for every frequency point and using a weighting function rejecting data point with low coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e0a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dv/v from the linear regression and the weighting function\n",
    "dvv, err, wf =get_dvv(freqs, t, WXamp, Wcoh, WXdt, lag_min, lag_max, freqmin=freqmin, freqmax=freqmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a296f-697c-4c95-b60e-ab6ef1071547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the results\n",
    "do_plot(t, WXamp, WXspec, WXangle, Wcoh, WXdt, freqs, coi, wf, pair, date, comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b565d-a771-4cdc-b5ac-2218f78fb034",
   "metadata": {},
   "source": [
    "Now you will find the figure in \"WCT/Figure\"\n",
    "![XWT example](Figures/XWT.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad2c73",
   "metadata": {},
   "source": [
    "# Kick it up a notch\n",
    "\n",
    "Now let's run the job for all dates and all components. This might take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799784bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for comp in comps:\n",
    "    sta1 = pair.split(\":\")[0]\n",
    "    sta2 = pair.split(\":\")[1]\n",
    "    ref_file = \"STACKS/{:02d}/REF/{}/{}_{}.MSEED\".format(filterid,\n",
    "                                                      comp,\n",
    "                                                      sta1,\n",
    "                                                      sta2)    \n",
    "    dvv_list = []\n",
    "    err_list = []\n",
    "    data_dates = []\n",
    "    cur_steps=mov_stack\n",
    "    dates=pd.date_range(start, end,freq=\"D\")\n",
    "    date_select = dates\n",
    "    if not os.path.isfile(ref_file):\n",
    "        print(\"Ref file {} does not exist.\".format(ref_file))\n",
    "        continue\n",
    "    sref=read(ref_file)\n",
    "    pbar = tqdm.tqdm(date_select, desc=\"Extracting CCF for pair {}\".format(pair))\n",
    "    for date in pbar:\n",
    "        ref = sref.copy()[0].data\n",
    "        pbar.set_description(\"Working on CCF pair {} on {}\".format(pair, date.date()))\n",
    "        fn2 = \"STACKS/{:02d}/{:03d}_DAYS/{}/{}_{}/{}.MSEED\".format(filterid,cur_steps,comp,\n",
    "                                                           sta1,\n",
    "                                                           sta2,\n",
    "                                                           date.date())     \n",
    "\n",
    "        if not os.path.isfile(fn2):\n",
    "            pbar.set_description(\"File {} does not exist.\".format(fn2))\n",
    "            continue   \n",
    "        current = read(fn2)[0].data\n",
    "        t = read(fn2)[0].times()-120\n",
    "        ori_waveform = (ref/ref.max()) #TODO make normalisation optional\n",
    "        new_waveform = (current/current.max())\n",
    "\n",
    "\n",
    "        WXamp, WXspec, WXangle, Wcoh, WXdt, freqs, coi = xwt(ori_waveform, new_waveform, fs, 3, 0.25, 10, freqmin, freqmax, 400)# TODO get freq lims from db \n",
    "        dvv, err, wf =get_dvv(freqs, t, WXamp, Wcoh, WXdt, lag_min, lag_max, freqmin=freqmin, freqmax=freqmax)\n",
    "        dvv_list.append(dvv)\n",
    "        err_list.append(err)\n",
    "        data_dates.append(date)\n",
    "        if plot:\n",
    "            do_plot(t, WXamp, WXspec, WXangle, Wcoh, WXdt, freqs, coi, wf, pair, date, comp)\n",
    "\n",
    "    if len(dvv_list)>1: # Check if the list has more than 1 measurement to save it\n",
    "        #inx = np.where((freqs>=freqmin) & (freqs<=freqmax)) # Select a new frequency range\n",
    "        dvv_df = pd.DataFrame(columns=freqs, index=data_dates)\n",
    "        err_df = pd.DataFrame(columns=freqs, index=data_dates)\n",
    "        pbar = tqdm.tqdm(data_dates, desc=\"Formating the DataFrame\")\n",
    "        for i, date in enumerate(pbar):\n",
    "            dvv_df.iloc[i]=dvv_list[i]\n",
    "            err_df.iloc[i]=err_list[i]\n",
    "            pbar.set_description(\"Formating the DataFrame for {}\".format(date.date()))\n",
    "        if saveit:\n",
    "            if not os.path.isdir(\"WCT\"):\n",
    "                os.makedirs(\"WCT\")\n",
    "            dfn = \"{} {}_ {} - {}.pkl\".format(pair.replace(\":\",\"_\"),comp,str(dvv_df.index[0].date()),str(dvv_df.index[-1].date()))\n",
    "            efn = \"Err {} {}_ {} - {}.pkl\".format(pair.replace(\":\",\"_\"),comp,str(dvv_df.index[0].date()),str(dvv_df.index[-1].date()))\n",
    "            path = os.path.join(\"WCT\",dfn)\n",
    "            epath = os.path.join(\"WCT\",efn)\n",
    "            dvv_df.to_pickle(path)    # Save dvv\n",
    "            err_df.to_pickle(epath)\n",
    "    elif saveit:\n",
    "        print(\"Not enough measurements to save...\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8a508-ce85-484f-acc6-05fa6d99ba09",
   "metadata": {},
   "source": [
    "# Computer says no...\n",
    "\n",
    "If you feel like your compute is taking too long to process even on one component combination, we left the solutions in the ./WTC/BAK folder.\n",
    "To use them, you can copy them to ./WTC using the command below and move on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87e6ac-e524-49cd-aab6-aafb05d4bb7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Works for all, including windows\n",
    "import glob\n",
    "import shutil\n",
    "dest_dir = \"WCT/\"\n",
    "for file in glob.glob('WCT/BAK/*'):\n",
    "    print(file)\n",
    "    shutil.copy(file, dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works for Linux and MAC\n",
    "!cp WCT/BAK/* WCT/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f64af73-5293-40ee-a8b2-8e4a103481b0",
   "metadata": {},
   "source": [
    "When you are happy with the processing (should be near 2200 files processed for each component combination if you did not make any compromises) move on to plotting the result of your labor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
