{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f34b8d",
   "metadata": {},
   "source": [
    "# Minionology: HVSR with seismic nodes\n",
    "### Skience2023 practical on HVSR, node installation, applications, Geopsy, continuous data analysis\n",
    "\n",
    "##### Authors:\n",
    "* Koen Van Noten ([@KoenVanNoten](https://github.com/KoenVanNoten))\n",
    "* Thomas Lecocq ([@seismotom](https://github.com/ThomasLecocq)\n",
    "\n",
    "## Exercise 3:  \n",
    "## The nature of the Bayrischzell valley infill\n",
    "In this exercise, we will do a similar analysis as in Brussels (ex1), but now with data gathered along the Leitzach river valley, W of Bayrischzell. These seismic dataset was gathered between Sunday 27/02/2023 and Monday 28/03/2023 by Koen, Tom and Raph who installed 12 SmartSolo nodes across the valley. See red dashed line in the figure below.\n",
    "\n",
    "##### Purpose\n",
    "Analyse the ambient noise recordings either using Geopsy manually and making a cross-profile (see ex 1), or using an automatic analysis (ex. 2) to investigate the valley infill.\n",
    "\n",
    "##### Data interpretation\n",
    "To make an interpretation of the data, it is useful to have a look to the local geological map of Bayrischzel, which can be consulted here: \n",
    "https://www.bestellen.bayern.de/application/pictureSrv?SID=3274027&ACTIONxSESSxSHOWPIC(BILDxKEY:%2712052%27,BILDxCLASS:%27Artikel%27,BILDxTYPE:%27BildGross%27)=Z\n",
    "\n",
    "and here:\n",
    "https://www.bestellen.bayern.de/application/pictureSrv?SID=3274027&ACTIONxSESSxSHOWPIC(BILDxKEY:%2712052%27,BILDxCLASS:%27Artikel%27,BILDxTYPE:%27BildGross%27)=Z\n",
    "\n",
    "__Infill Geology__:\n",
    "* Postglacial Schoffer (Sand und Schoffer)\n",
    "* Local morene mitt Wallform\n",
    "\n",
    "__Bedrock Geology__:\n",
    "* Hauptdolomite (HD)\n",
    "\n",
    "<img src=\"Figures/Bayrischzell valley.png\" width=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68bfb12",
   "metadata": {},
   "source": [
    "The seismic dataset gathered Bayrischzell can be downloaded here:\n",
    "__TO DO__\n",
    "\n",
    "Before playing with the HVSR data, let's have a look on data gathered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fce1cf",
   "metadata": {},
   "source": [
    "### 0. Prequel: elevation data\n",
    "First, read the topography cross-profile _'Barischzell_crossprofile.csv'_ in folder __in HVSR_ex3_Bay\\__  \n",
    "\n",
    "Plot it along distance using the cumulative distance with respect to 1st point. The first point is the most northern point in the valley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = r'HVSR_ex3_Bay\\Barischzell_crossprofile.csv'\n",
    "df_profile = pd.read_csv(profile)\n",
    "lat_profile = df_profile['Lat']\n",
    "lon_profile = df_profile['Lon']\n",
    "Z_profile = df_profile[\"Z\"]\n",
    "\n",
    "# compute inter_distances\n",
    "inter_distances = []\n",
    "for nr in np.arange(0,len(df_profile)-1,1):\n",
    "    inter_distances.append(gps2dist_azimuth(lat_profile[nr], lon_profile[nr], lat_profile[nr+1],lon_profile[nr+1])[0])\n",
    "inter_distances = pd.Series(inter_distances)\n",
    "\n",
    "# open a series, but start with distance 0\n",
    "cumul = pd.Series([0])\n",
    "# add the cumulative distances to the cumul list\n",
    "d_cumul_profile = pd.concat([cumul,inter_distances.cumsum()], ignore_index = True)\n",
    "#d_cumul\n",
    "\n",
    "#inter_distances\n",
    "fig, ax= plt.subplots(figsize=(15,5))\n",
    "plt.plot(d_cumul_profile,Z)\n",
    "plt.ylabel('Altitude [m]', fontsize = 14)\n",
    "plt.xlabel('Distance [m]', fontsize = 14)\n",
    "plt.title('Bayrischzell HV profile', fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac93130",
   "metadata": {},
   "source": [
    "### Let's start\n",
    "Activate the necessary modules/def's first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5076220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import obspy\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mcoll\n",
    "from obspy.imaging.scripts.scan import Scanner\n",
    "from obspy.geodetics.base import gps2dist_azimuth \n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# run the previous notebook\n",
    "%run 00_HVSR_Minionology_definitions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2514195",
   "metadata": {},
   "source": [
    "Scan the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner = Scanner()\n",
    "scanner.parse(\"HVSR_ex3_Bay\")  \n",
    "scanner.plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff3d06",
   "metadata": {},
   "source": [
    "### 1. Manual geopsy HVSR dataprocessing\n",
    "In the first solution, solve the HV data in Geopsy manually and save the .hv file\n",
    "\n",
    "Use following parameters (or play with them):\n",
    "* __Length__: 120s\n",
    "* __Overlap__: 50%\n",
    "* __Relative treshhold__: 70%\n",
    "* __Taper__: 5% Tukey\n",
    "* __Konno-Omachi smoothing__: 40%\n",
    "* __Squared Average__\n",
    "* __Output__: 0.20Hz - 50 Hz\n",
    "* __Step Count__: 500\n",
    "\n",
    "After computation, manually clean the H/V curve by _Reject time windows_ (right click on the graph) and select those curves that deviate from the mean curve. Then recompute the H/V curve by pressing the black arrow next to _select_ -> _clear_ -> _Remove grayed_ and press _Start_ again.\n",
    "\n",
    "To save the .HV results do a _Tools_ -> _Save Results_ and save it in the  __HVSR_ex3_Bay\\Analysed__ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871a405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feab432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbdbbe27",
   "metadata": {},
   "source": [
    "### 2. Reading theBayrischzell HVSR database file\n",
    "All the node metadata information has been preloaded in an HVSR database file. The HVSR database file is available in the __HVSR_ex3_Bay__ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "HV_database = r'HVSR_ex3_Bay\\HVSR_database_Bayrischzell.csv'\n",
    "HV_basefolder, HV_db_name = os.path.split(HV_database)[0], os.path.split(HV_database)[1]\n",
    "print(HV_basefolder, HV_db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19c631",
   "metadata": {},
   "source": [
    "Load the HVSR database into a pandas database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_HVSR = pd.read_csv(HV_database, encoding='latin')\n",
    "db_HVSR.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef408583",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0232c443",
   "metadata": {},
   "source": [
    "Compute the distance of each node to the first node using the Obspy _obspy.geodetics.base.gps2dist_azimuth_ function and load it to an inter_distance array\n",
    "\n",
    "inter_distance = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872837a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.geodetics.base import gps2dist_azimuth \n",
    "\n",
    "lat = db_HVSR['Lat']\n",
    "lon = db_HVSR['Lon']\n",
    "\n",
    "inter_distances = []\n",
    "for nr in np.arange(0,len(db_HVSR)-1,1):\n",
    "    inter_distances.append(gps2dist_azimuth(lat[nr], lon[nr], lat[nr+1],lon[nr+1])[0])\n",
    "inter_distances = pd.Series(inter_distances)\n",
    "inter_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c952093",
   "metadata": {},
   "source": [
    "Make a cumulative distance series (d_cumul) with _pd.series.cumsum()_\n",
    "\n",
    "d_cumul value of the first node is distance to the first node of the Bayrischzell profile. Compute this distance and add that  as first distance to d_cumul with pd.concat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e28ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#distance of first node to first point of the profile\n",
    "d_first_node = gps2dist_azimuth(lat_profile[0], lon_profile[0], lat[0],lon[0])[0]\n",
    "print(lat_profile[0], lon_profile[0], lat[0], lon[0])\n",
    "print(d_first_node)\n",
    "\n",
    "# open a series, and add the first node distance to it \n",
    "cumul = pd.Series(d_first_node)\n",
    "\n",
    "# add the cumulative distances + d_first_node to the cumul list\n",
    "d_cumul = pd.concat([cumul,inter_distances.cumsum()+d_first_node], ignore_index = True)\n",
    "d_cumul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550bfd3",
   "metadata": {},
   "source": [
    "Add the cumulative distance column to the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cab818",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_HVSR['d_cumul'] = d_cumul\n",
    "db_HVSR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e7cb5",
   "metadata": {},
   "source": [
    "### 3. Filling the HVSR database file with .hv data\n",
    "\n",
    "By using Geopsy, the HVSR data has been saved as .hv files in the _HVSR_ex1_Bru\\Analysed_ folder. We will extract and add it to the HVSR database. The script in below (Van Noten _et al._ 2022) reads the HVSR database and extracts all the necessary data in the .hv files being:\n",
    "\n",
    "* __f0 min__: f0_win/stddev (from GEOPSY)\n",
    "* __f0_win__: average resonance frequency by taking the f0 of each individual window and averaging all f0 values from these windows (from GEOPSY)\n",
    "* __f0 average__: scanning the average curve and identifying the frequency at which the maximum amplitude occurs (from GEOPSY)\n",
    "* __f0_ip__: resonance frequency computed after interpolating the HV-Amplitude graph using python. This is useful if you forgot to adapt the Step Count\n",
    "* __f0_ip_diff__: difference between f0_ip and f0_win\n",
    "* __error__: standard deviation on f0 (from GEOPSY)\n",
    "* __f0 max__: f0_win.stddev (from GEOPSY)\n",
    "* __A0__: maximum amplitude (from GEOPSY)\n",
    "* __nw__: number of windows (from GEOPSY)\n",
    "\n",
    "Reference:\n",
    "https://github.com/KoenVanNoten/HVSR_to_virtual_borehole/blob/master/Get%20f0s%20from%20geopsy%20hv%20files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b75111",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_file = HV_database\n",
    "db_HVSR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff928283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the database file in which all the names of the .hv measurements are stored\n",
    "HV_folder = os.path.join(HV_basefolder, 'Analysed')  #folder containing all .hv data\n",
    "\n",
    "# create a copy of the HVSR database \n",
    "outputfile = db_HVSR\n",
    "\n",
    "#### Initializing empty columns that need to be filled from the Geopsy .hv files\n",
    "for _ in [\"f0_min\", \"f0_win\", \"f0_average\", \"f0_ip\", \"f0_ip_diff\", \"error\", \"f0_max\", \"A0\", \"nw\"]:\n",
    "    outputfile[_] = 0.\n",
    "\n",
    "output = []\n",
    "\n",
    "#### loop through each .hv datafile\n",
    "for id, row in outputfile.iterrows():\n",
    "    HV_file = os.path.join(HV_folder, row[\"ID\"] + \".hv\")\n",
    "    print(HV_file)\n",
    "    df = pd.read_csv(HV_file, nrows=5, skiprows=1, header=None) #opening the .hv file\n",
    "    rows = [\"num\", \"f0_avg\", \"num_f0\", \"f0s\", \"peak_amp\"] #define rows to write in the output file\n",
    "    data = {}\n",
    "    for row in rows:\n",
    "        data[row] = \"\"\n",
    "    delims = [\"=\", \"\\t\", \"=\", \"\\t\", \"\\t\"]\n",
    "    for id2, item in df.iterrows():\n",
    "        XXX = item[0].split(delims[id2])\n",
    "        data[rows[id2]] = np.asarray(XXX[1:], dtype=float).flatten()\n",
    "    data[\"f0_win\"], data[\"f_min\"], data[\"f_max\"] = data[\"f0s\"]\n",
    "    del data[\"f0s\"]\n",
    "    data[\"error\"] = data[\"f0_win\"] - data[\"f_min\"]\n",
    "    data[\"f0_avg\"] = data[\"f0_avg\"][0]\n",
    "    data[\"peak_amp\"] = data[\"peak_amp\"][0]\n",
    "    data[\"num\"] = data[\"num\"][0]\n",
    "    data[\"num_f0\"] = data[\"num_f0\"][0]\n",
    "\n",
    "    # In the default setting, Geopsy only exports 100 frequency-amplitude samples for the computed HVSR curve.\n",
    "    # One can increase this number by:\n",
    "    #   - either increasing the sample numbers in geopsy (max = 9999)\n",
    "    #   - or by interpolating between the samples and improve the accuracy of picking f0 (this script)\n",
    "    # Increasing the samples in Geopsy to 9999 gives the same results, but one might have forgotten to do this while processing\n",
    "    # so this interpolation offers a nice twist to solve this.\n",
    "\n",
    "    # The part in below executes the interpolation up to 15000 samples\n",
    "    # See paper Van Noten et al. for more information. Same method is applied in the HVSR_to_virtual_borehole module\n",
    "    HV_data = np.genfromtxt(HV_file, delimiter='\\t', usecols=(0, 1))\n",
    "    f_orig = HV_data[:, 0] #original frequency data\n",
    "    print(\"nr of samples:\", len(f_orig))\n",
    "    A_orig = HV_data[:, 1] #original amplitude data\n",
    "    func = interp1d(f_orig, A_orig, 'cubic') #IN\n",
    "    f_new = np.linspace(f_orig[0], f_orig[-1], 15000) #interpolation for 15000 samples\n",
    "    A_new = func(f_new) #defining the function for the new Amplitude\n",
    "    maxx = np.argmax(A_new)\n",
    "\n",
    "    # With the interpolated data new columns can be calculated to compare the interpolated values and the ones provided by Geopsy\n",
    "    f0_ip_diff = f_new[maxx] - data[\"f0_win\"] #difference between f0_interpolated and f0_geopsy\n",
    "\n",
    "    #write all data to the database file\n",
    "    outputfile.loc[id, \"f0_min\"] = data[\"f_min\"] #f0 min\n",
    "    print(\"f0_min:\", round(data[\"f_min\"],3), \"Hz\")\n",
    "    outputfile.loc[id, \"f0_win\"] = data[\"f0_win\"] #average f0 computed by averaging the peak f0 values of all individual windows\n",
    "    print(\"f0_win:\", round(data[\"f0_win\"],3), \"Hz\")\n",
    "    outputfile.loc[id, \"f0_average\"] = data[\"f0_avg\"] #f0 corresponding to the maximum amplitude of the average f0 - Amplitude curve\n",
    "    print(\"f0_average:\", round(data[\"f0_avg\"],3), \"Hz\")\n",
    "    outputfile.loc[id, \"f0_ip\"] = f_new[maxx] #interpolated f0 from 15000 samples\n",
    "    print(\"f0_ip:\", round(f_new[maxx],3), \"Hz\")\n",
    "    outputfile.loc[id, \"f0_ip_diff\"] = f0_ip_diff #difference between f0_interpolated and f0_win\n",
    "    print (\"f0_ip_diff:\", round(f0_ip_diff,3), \"Hz\")\n",
    "    outputfile.loc[id, \"error\"] = data[\"error\"] #error on f0_win in Geopsy\n",
    "    print(\"error:\", round(data[\"error\"],3), \"Hz\")\n",
    "    outputfile.loc[id, \"f0_max\"] = data[\"f_max\"] #f0 max\n",
    "    print(\"f0_max:\", round(data[\"f_max\"],3), \"Hz\")\n",
    "    outputfile.loc[id, \"A0\"] = data[\"peak_amp\"] #A0\n",
    "    print(\"A0:\", round(data[\"peak_amp\"],3))\n",
    "    outputfile.loc[id, \"nw\"] = data[\"num\"] #number of windows used to compute f0\n",
    "    print(\"nw:\", int(data[\"num\"]), \"windows\")\n",
    "    print('')\n",
    "\n",
    "outputfile.to_csv(out_filespec, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c24671f",
   "metadata": {},
   "source": [
    "We can now check how the map looks like and color it by whatever parameter you want. e.g. f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the HV plot\n",
    "fig, ax= plt.subplots()\n",
    "#Checking how the profile looks like - Use the Belgian Lambert72 projection column \n",
    "scatter = plt.scatter(db_HVSR['UTM-X'], db_HVSR['UTM-Y'], c=db_HVSR['f0_average'],  cmap = 'viridis')\n",
    "cb = plt.colorbar(scatter, orientation='vertical')\n",
    "cb.set_label('Resonance frequency', backgroundcolor = 'white')\n",
    "plt.scatter(db_HVSR['UTM-X'][0], db_HVSR['UTM-Y'][0], c='red', label='first node: %s'%(db_HVSR[\"ID\"][0]))\n",
    "ax.axis('equal')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cefcd07",
   "metadata": {},
   "source": [
    "### 4. Creating an HVSR cross-profile\n",
    "We want to create an HVSR profile showing the variation of resonance frequency along the profile where the nodes where installed. We will create this distance plot using the cumulative distances computed earlier. Herefore we need to link each .hv file with its cumulative distance point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b7b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot\n",
    "fig, ax= plt.subplots(figsize=(16,6))\n",
    "\n",
    "# We will loop over the HVSR database using pandas groupby to plot each HV profile along the distance. \n",
    "for id, group in db_HVSR.groupby(\"H/V\"):\n",
    "    #create empty arrays to get data\n",
    "    freqs= []\n",
    "    a0s = []\n",
    "    amins = []\n",
    "    amaxs = []\n",
    "    max_amp = 0\n",
    "    \n",
    "    # loop over the .hv files\n",
    "    for id,line in group.iterrows():\n",
    "        ID = line.ID + \".hv\"\n",
    "        HV_file = os.path.join(HV_db_folder, 'Analysed', ID)\n",
    "        \n",
    "        ### load necessary data\n",
    "        Freq, A0, A_min, A_max = read_HV(HV_file)\n",
    "        plt.plot(A0, Freq, c='grey', alpha = 0.5)\n",
    "\n",
    "        # scatterplot a dot on f0 and A0      \n",
    "        f0_curve, A0_curve, A0_min_curve, A0_max_curve = get_params_from_HV_curve(HV_file)\n",
    "        plt.scatter(A0_curve, f0_curve, c = 'red', edgecolors = 'red', alpha = 0.5, zorder = 10, \n",
    "            label='$f_0$: %s Hz \\n$A_0$: %s'%(round(Fmax_f0,2), round(Amax_f0,1)))\n",
    "        \n",
    "        # or get it from the HV file and plot\n",
    "        f0_avg, f0_win, error, A0, nw_avg, nw_win, f_min, f_max = get_params_from_HV(HV_file)\n",
    "        plt.scatter(A0, f0_avg, c = 'red', edgecolors = 'red', alpha = 0.5, zorder = 10 ,\n",
    "                    label='$f_0$: %s Hz \\n$A_0$: %s'%(round(Fmax_f0,2), round(Amax_f0,1)))\n",
    "        \n",
    "        \n",
    "ax.set_yscale('log')\n",
    "plt.xlabel('H/V amplitude', fontsize = 14)\n",
    "plt.ylabel('Frequency ([Hz])', fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d03a3",
   "metadata": {},
   "source": [
    "### Create the cross-profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e610ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HV_database = r'HVSR_ex1_Bru\\HVSR_database_TouretTaxis_f0_from_hv.csv'\n",
    "db_HVSR = pd.read_csv(HV_database, encoding='latin')\n",
    "\n",
    "### Choose which profile you want: \n",
    "freq_profile = False ## frequency profile of all HVs\n",
    "\n",
    "depth_profile_fixed = False #depth profile with frequency converted to depth with a fixed velocity \n",
    "Vs = 400 #m/s\n",
    "\n",
    "depth_profile_powerlaw = True #depth profile with frequency converted to depth using a powerlaw equation\n",
    "a_pw = 88.631     # a value of the powerlaw\n",
    "b_pw = -1.683    # b value of the powerlaw\n",
    "\n",
    "# create plot\n",
    "fig, ax= plt.subplots(figsize=(16,6))\n",
    "\n",
    "# Create empty arrays to get all the HV data\n",
    "Freqs= []\n",
    "A0s = []\n",
    "Amins = []\n",
    "Amaxs = []\n",
    "f0s_db = []\n",
    "A0s_db = []\n",
    "max_amp = 0\n",
    " \n",
    "# loop over the .hv files\n",
    "for id, line in db_HVSR.iterrows():\n",
    "        ID = line.ID + \".hv\"\n",
    "        HV_file = os.path.join(HV_db_folder, 'Analysed', ID)\n",
    "        \n",
    "        ### load necessary data\n",
    "        Freq, A0, A_min, A_max = read_HV(HV_file)\n",
    "        Freqs.append(Freq)\n",
    "        A0s.append(A0)\n",
    "        Amins.append(Amin)\n",
    "        Amaxs.append(Amax)\n",
    "        # get the maximum amplitude of all HV files to normalise the max amplitude later\n",
    "        if A0.max() > max_amp:\n",
    "            max_amp = A0.max()\n",
    "        f0s_db.append(line.f0_average)\n",
    "        A0s_db.append(line.A0)\n",
    "        \n",
    "# Normalise the amplitudes and min and max (to potentially plot the error on f0)\n",
    "A0s = pd.Series(A0s)/max_amp\n",
    "Amins = pd.Series(Amins)/max_amp\n",
    "Amaxs = pd.Series(Amaxs)/max_amp\n",
    "\n",
    "# create a pandas db for plotting all curves\n",
    "db_HVs = pd.DataFrame({'Freqs':Freqs, 'A0s':A0s, 'Amins':Amins, 'Amaxs':Amaxs, \n",
    "                       'd_cumul':db_HVSR[\"d_cumul\"], 'Z':db_HVSR[\"Z\"]})\n",
    "for k, _ in db_HVs.iterrows():        \n",
    "        # convert amplitudes to distances\n",
    "        arrays = []\n",
    "        for ar in [_.A0s, _.Amins, _.Amaxs]:\n",
    "            ar *= 80\n",
    "            ar += _.d_cumul\n",
    "            arrays.append(ar)\n",
    "        # if you want a frequency plot y = frequency values        \n",
    "        if freq_profile:\n",
    "            y = _.Freqs\n",
    "        \n",
    "        # if you want a depth profile with a fixed Vs,  y = Vs / 4*f0            \n",
    "        if depth_profile_fixed:\n",
    "            y = Vs / (_.Freqs * 4)\n",
    "            y= _.Z - y\n",
    "    \n",
    "        # if you want a depth profile with a powerlaw conversion,  y = powerlaw            \n",
    "        if depth_profile_powerlaw:\n",
    "            y = a_pw * np.power(_.Freqs,b_pw)\n",
    "            y= _.Z - y\n",
    "        \n",
    "        # color the lines\n",
    "        colorline(arrays[0], y, _.A0s, cmap='viridis', linewidth=5)\n",
    "        colorbar = colorline(arrays[0], y, _.A0s, cmap='viridis', linewidth=5)        \n",
    "        \n",
    "        # scatterplot a dot on f0 and A0\n",
    "        Amax_f0 = np.max(arrays[0])\n",
    "        Fmax_f0 = y[np.argmax(arrays[0])]\n",
    "        plt.scatter(Amax_f0, Fmax_f0, c = 'red', edgecolors = 'red', alpha = 0.5, zorder = 10)\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.ylim(0.5,100)\n",
    "\n",
    "if freq_profile:\n",
    "        plt.ylabel(\"Frequency [Hz]\",  fontsize=14)\n",
    "        ax.set_yscale('log')\n",
    "else:\n",
    "        plt.ylabel(\"Depth [m]\",  fontsize=14)\n",
    "        plt.ylim(-150,30)\n",
    "\n",
    "plt.xlabel(\"Distance [m] & Normalized Amplitude\",  fontsize=14)\n",
    "id = '%s '%(db_HVSR[\"Comment\"][0].split('_')[0])\n",
    "plt.title('H/V spectral ratio analysis - %s'%id,  fontsize=16)\n",
    "plt.xlim(0,1600)\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
