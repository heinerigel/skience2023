{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f34b8d",
   "metadata": {},
   "source": [
    "# Minionology: HVSR with seismic nodes\n",
    "### Skience2023 practical on HVSR, node installation, applications, Geopsy, continuous data analysis\n",
    "\n",
    "##### Minion Gru's:\n",
    "* Koen Van Noten ([@KoenVanNoten](https://github.com/KoenVanNoten))\n",
    "* Thomas Lecocq ([@seismotom](https://github.com/ThomasLecocq))\n",
    "* Raphael De Plaen\n",
    "\n",
    "## Exercise 3:  \n",
    "## The nature of the Bayrischzell valley infill\n",
    "In this exercise, we will do a similar analysis as in Brussels (ex1), but now with data gathered along the Leitzach river valley, W of Bayrischzell. These seismic dataset was gathered between Sunday 27/02/2023 and Monday 28/03/2023 by Koen, Tom and Raph who installed 12 SmartSolo nodes across the valley. See red dashed line in the figure below.\n",
    "\n",
    "##### Purpose\n",
    "Analyse the ambient noise recordings either using Geopsy manually and making a cross-profile (see ex 1), or using an automatic analysis (ex. 2) to investigate the valley infill.\n",
    "\n",
    "##### Data interpretation\n",
    "To make an interpretation of the data, it is useful to have a look to the local geological map of Bayrischzel, which can be consulted here: \n",
    "https://www.bestellen.bayern.de/application/pictureSrv?SID=3274027&ACTIONxSESSxSHOWPIC(BILDxKEY:%2712052%27,BILDxCLASS:%27Artikel%27,BILDxTYPE:%27BildGross%27)=Z\n",
    "\n",
    "and here:\n",
    "https://geoportal.bayern.de/bayernatlas/?lang=de&topic=umwe&bgLayer=atkis&catalogNodes=110&layers=8885cab8-d186-4bfd-b61e-d419457649e8&E=723374.18&N=5285515.14&zoom=10&layers_opacity=0.6\n",
    "\n",
    "__Infill Geology__:\n",
    "* Postglacial Schoffer (Sand und Schoffer)\n",
    "* Local morene mitt Wallform\n",
    "\n",
    "__Bedrock Geology__:\n",
    "* Hauptdolomite (HD)\n",
    "\n",
    "<img src=\"Figures/Bayrischzell valley.png\" width=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68bfb12",
   "metadata": {},
   "source": [
    "The seismic dataset gathered Bayrischzell can be downloaded here:\n",
    "__TO DO__\n",
    "\n",
    "Before playing with the HVSR data, let's have a look on data gathered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fce1cf",
   "metadata": {},
   "source": [
    "### 0. Prequel: elevation data\n",
    "First, read the topography cross-profile _'Barischzell_crossprofile.csv'_ in folder __in HVSR_ex3_Bay\\__  \n",
    "\n",
    "Plot it along distance using the cumulative distance with respect to 1st point. The first point is the most northern point in the valley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5076220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import obspy\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as mcoll\n",
    "import matplotlib.ticker as mticker\n",
    "from obspy import read\n",
    "from obspy.imaging.scripts.scan import Scanner\n",
    "from obspy.geodetics.base import gps2dist_azimuth \n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# run the previous notebook\n",
    "%run 00_HVSR_Minionology_definitions.ipynb\n",
    "\n",
    "# use ipython notebook in a wider screen\n",
    "from IPython.display import display, HTML # Widen the view\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = r'HVSR_ex3_Bay\\Barischzell_crossprofile.csv'\n",
    "df_profile = pd.read_csv(profile)\n",
    "lat_profile = df_profile['Lat']\n",
    "lon_profile = df_profile['Lon']\n",
    "Z_profile = df_profile[\"Z\"]\n",
    "\n",
    "# compute inter_distances\n",
    "inter_distances = []\n",
    "for nr in np.arange(0,len(df_profile)-1,1):\n",
    "    inter_distances.append(gps2dist_azimuth(lat_profile[nr], lon_profile[nr], lat_profile[nr+1],lon_profile[nr+1])[0])\n",
    "inter_distances = pd.Series(inter_distances)\n",
    "\n",
    "# open a series, but start with distance 0\n",
    "cumul = pd.Series([0])\n",
    "# add the cumulative distances to the cumul list\n",
    "d_cumul_profile = pd.concat([cumul,inter_distances.cumsum()], ignore_index = True)\n",
    "#d_cumul\n",
    "\n",
    "#inter_distances\n",
    "fig, ax= plt.subplots(figsize=(15,5))\n",
    "plt.plot(d_cumul_profile,Z_profile)\n",
    "plt.ylabel('Altitude [m]', fontsize = 14)\n",
    "plt.xlabel('Distance [m]', fontsize = 14)\n",
    "plt.title('Bayrischzell HV profile', fontsize = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac93130",
   "metadata": {},
   "source": [
    "### Let's start\n",
    "Activate the necessary modules/def's first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2514195",
   "metadata": {},
   "source": [
    "Scan the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner = Scanner()\n",
    "scanner.parse(\"HVSR_ex3_Bay\")  \n",
    "scanner.plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff3d06",
   "metadata": {},
   "source": [
    "### 1. Manual geopsy HVSR dataprocessing\n",
    "In the first solution, solve the HV data in Geopsy manually and save the .hv file\n",
    "\n",
    "Use following parameters (or play with them):\n",
    "* __Length__: 120s\n",
    "* __Overlap__: 50%\n",
    "* __Relative treshhold__: 70%\n",
    "* __Taper__: 5% Tukey\n",
    "* __Konno-Omachi smoothing__: 40%\n",
    "* __Squared Average__\n",
    "* __Output__: 0.20Hz - 50 Hz\n",
    "* __Step Count__: 500\n",
    "\n",
    "After computation, manually clean the H/V curve by _Reject time windows_ (right click on the graph) and select those curves that deviate from the mean curve. Then recompute the H/V curve by pressing the black arrow next to _select_ -> _clear_ -> _Remove grayed_ and press _Start_ again.\n",
    "\n",
    "To save the .HV results do a _Tools_ -> _Save Results_ and save it in the  __HVSR_ex3_Bay\\Analysed__ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871a405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feab432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbdbbe27",
   "metadata": {},
   "source": [
    "### 2. Reading theBayrischzell HVSR database file\n",
    "All the node metadata information has been preloaded in an HVSR database file. The HVSR database file is available in the __HVSR_ex3_Bay__ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "HV_database = r'HVSR_ex3_Bay\\HVSR_database_Bayrischzell.csv'\n",
    "HV_db_folder, HV_db_name = os.path.split(HV_database)[0], os.path.split(HV_database)[1]\n",
    "print(HV_db_folder, HV_db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19c631",
   "metadata": {},
   "source": [
    "Load the HVSR database into a pandas database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_HVSR = pd.read_csv(HV_database, encoding='latin')\n",
    "db_HVSR.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef408583",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0232c443",
   "metadata": {},
   "source": [
    "Compute the distance of each node to the first node using the Obspy _obspy.geodetics.base.gps2dist_azimuth_ function and load it to an inter_distance array\n",
    "\n",
    "inter_distance = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872837a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.geodetics.base import gps2dist_azimuth \n",
    "\n",
    "lat = db_HVSR['Lat']\n",
    "lon = db_HVSR['Lon']\n",
    "\n",
    "inter_distances = []\n",
    "for nr in np.arange(0,len(db_HVSR)-1,1):\n",
    "    inter_distances.append(gps2dist_azimuth(lat[nr], lon[nr], lat[nr+1],lon[nr+1])[0])\n",
    "inter_distances = pd.Series(inter_distances)\n",
    "inter_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c952093",
   "metadata": {},
   "source": [
    "Make a cumulative distance series (d_cumul) with _pd.series.cumsum()_\n",
    "\n",
    "d_cumul value of the first node is distance to the first node of the Bayrischzell profile. Compute this distance and add that  as first distance to d_cumul with pd.concat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e28ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#distance of first node to first point of the profile\n",
    "d_first_node = gps2dist_azimuth(lat_profile[0], lon_profile[0], lat[0],lon[0])[0]\n",
    "\n",
    "# open a series, and add the first node distance to it \n",
    "cumul = pd.Series(d_first_node)\n",
    "\n",
    "# add the cumulative distances + d_first_node to the cumul list\n",
    "d_cumul = pd.concat([cumul,inter_distances.cumsum()+d_first_node], ignore_index = True)\n",
    "d_cumul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0550bfd3",
   "metadata": {},
   "source": [
    "Add the cumulative distance column to the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cab818",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_HVSR['d_cumul'] = d_cumul\n",
    "db_HVSR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e7cb5",
   "metadata": {},
   "source": [
    "### 3. Filling the HVSR database file with .hv data\n",
    "\n",
    "By using Geopsy, the HVSR data has been saved as .hv files in the _HVSR_ex1_Bru\\Analysed_ folder. We will extract and add it to the HVSR database. The script in below (Van Noten _et al._ 2022) reads the HVSR database and extracts all the necessary data in the .hv files being:\n",
    "\n",
    "* __f0 min__: f0_win/stddev (from GEOPSY)\n",
    "* __f0_win__: average resonance frequency by taking the f0 of each individual window and averaging all f0 values from these windows (from GEOPSY)\n",
    "* __f0 average__: scanning the average curve and identifying the frequency at which the maximum amplitude occurs (from GEOPSY)\n",
    "* __f0_ip__: resonance frequency computed after interpolating the HV-Amplitude graph using python. This is useful if you forgot to adapt the Step Count\n",
    "* __f0_ip_diff__: difference between f0_ip and f0_win\n",
    "* __error__: standard deviation on f0 (from GEOPSY)\n",
    "* __f0 max__: f0_win.stddev (from GEOPSY)\n",
    "* __A0__: maximum amplitude (from GEOPSY)\n",
    "* __nw__: number of windows (from GEOPSY)\n",
    "\n",
    "Reference:\n",
    "https://github.com/KoenVanNoten/HVSR_to_virtual_borehole/blob/master/Get%20f0s%20from%20geopsy%20hv%20files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b75111",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_HVSR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64675a5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read the database file in which all the names of the .hv measurements are stored\n",
    "HV_folder = os.path.join(HV_db_folder, 'Analysed')  #folder containing all .hv data\n",
    "\n",
    "#### Initializing empty columns that need to be filled from the Geopsy .hv files\n",
    "for _ in [\"f0_min\", \"f0_win\", \"f0_avg\", \"f0_int\", \"f0_int_diff\", \"error\", \"f0_max\", \"A0\", \"nw\"]:\n",
    "    db_HVSR[_] = 0.\n",
    "\n",
    "# specify a name for the HVSR + HV params database\n",
    "out_file = os.path.splitext(HV_db_name)[0] + \"_f0_from_hv.csv\"\n",
    "\n",
    "#### loop through each .hv datafile\n",
    "for id, row in db_HVSR.iterrows():\n",
    "    HV_file = os.path.join(HV_folder, row[\"ID\"] + \".hv\")\n",
    "    print(HV_file)\n",
    "    \n",
    "    # get all params from the HV file\n",
    "    f0_avg, f0_win, error, A0, nw_avg, nw_win, f_min, f_max = get_params_from_HV(HV_file)\n",
    "    \n",
    "    # get interpolated f0 and A0 from the HV file\n",
    "    f0_int, A0_int, f0_int_diff = get_interpolated_values_from_HV(HV_file, 15000, f0_win)\n",
    "\n",
    "    #write all data to the database file\n",
    "    #write_HVline_to_db?\n",
    "    write_HVline_to_db(db_HVSR, f_min, f0_win, f0_avg, f0_int, f0_int_diff, error, f_max, A0, nw_win)\n",
    "\n",
    "\n",
    "db_HVSR.to_csv(os.path.join(HV_db_folder, out_file), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff928283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c24671f",
   "metadata": {},
   "source": [
    "We can now check how the map looks like and color it by whatever parameter you want. e.g. f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the HV plot\n",
    "fig, ax= plt.subplots()\n",
    "#Checking how the profile looks like - Use the Belgian Lambert72 projection column \n",
    "scatter = plt.scatter(db_HVSR['UTM-X'], db_HVSR['UTM-Y'], c=db_HVSR['f0_avg'],  cmap = 'viridis')\n",
    "cb = plt.colorbar(scatter, orientation='vertical')\n",
    "cb.set_label('Resonance frequency', backgroundcolor = 'white')\n",
    "plt.scatter(db_HVSR['UTM-X'][0], db_HVSR['UTM-Y'][0], c='red', label='first node: %s'%(db_HVSR[\"ID\"][0]))\n",
    "ax.axis('equal')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cefcd07",
   "metadata": {},
   "source": [
    "### 4. Creating an HVSR cross-profile\n",
    "We want to create an HVSR profile showing the variation of resonance frequency along the profile where the nodes where installed. We will create this distance plot using the cumulative distances computed earlier. Herefore we need to link each .hv file with its cumulative distance point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b7b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot\n",
    "fig, ax= plt.subplots(figsize=(16,6))\n",
    "\n",
    "# We will loop over the HVSR database using pandas groupby to plot each HV profile along the distance. \n",
    "for id, group in db_HVSR.groupby(\"H/V\"):\n",
    "    \n",
    "    # loop over the .hv files\n",
    "    for id,line in group.iterrows():\n",
    "        ID = line.ID + \".hv\"\n",
    "        HV_file = os.path.join(HV_db_folder, 'Analysed', ID)\n",
    "        \n",
    "        ### load necessary data\n",
    "        Freq, A0, A_min, A_max = read_HV(HV_file)\n",
    "        plt.plot(A0, Freq, c='grey', alpha = 0.5)\n",
    "\n",
    "        # scatterplot a dot on f0 and A0      \n",
    "        f0_curve, A0_curve, A0_min_curve, A0_max_curve = get_params_from_HV_curve(HV_file)\n",
    "        plt.scatter(A0_curve, f0_curve, c = 'red', edgecolors = 'red', alpha = 0.5, zorder = 10, \n",
    "            label='$f_0$: %s Hz \\n$A_0$: %s'%(round(f0_curve,2), round(A0_max_curve,1)))\n",
    "        \n",
    "        # or scatterplot a dot on f0 and A0 from the HV file\n",
    "        f0_avg, f0_win, error, A0, nw_avg, nw_win, f_min, f_max = get_params_from_HV(HV_file)\n",
    "        plt.scatter(A0, f0_avg, c = 'red', edgecolors = 'green', alpha = 0.5, zorder = 10 ,\n",
    "                    label='$f_0$: %s Hz \\n$A_0$: %s'%(round(f0_curve,2), round(A0_max_curve,1)))\n",
    "\n",
    "        # or get the values from a partial part of the HV curve and scatterplot\n",
    "        f_range, A_range, A_min_range, A_max_range = get_params_from_partial_HV_curve(HV_file, 10 , 100)\n",
    "        plt.scatter(A_range, f_range, c = 'blue', edgecolors = 'grey', alpha = 0.8, zorder = 10, \n",
    "                    label='$f_0$: %s Hz \\n$A_0$: %s'%(round(f_range,2), round(A_range,1)))\n",
    "ax.set_yscale('log')\n",
    "plt.xlabel('H/V amplitude', fontsize = 14)\n",
    "plt.ylabel('Frequency ([Hz])', fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f320f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's read one\n",
    "HV_file = r'HVSR_ex3_Bay\\Analysed\\BE_01974.hv'\n",
    "\n",
    "# create the plot\n",
    "fig, ax= plt.subplots(figsize=(10,5))\n",
    "\n",
    "# get the HV values and plot\n",
    "Freq, A, A_min, A_max = read_HV(HV_file)\n",
    "plot_HV(Freq, A, A_min, A_max, f0_curve, A0_curve,  A0_min_curve, A0_max_curve, 'manual', 'blue')\n",
    "\n",
    "# or get the values from a partial part of the HV curve and scatterplot\n",
    "f_range, A_range, A_min_range, A_max_range = get_params_from_partial_HV_curve(HV_file, 1 , 3)\n",
    "plt.scatter(A_range, f_range, c = 'blue', edgecolors = 'grey', alpha = 0.8, zorder = 10, \n",
    "                    label='$f_0$: %s Hz \\n$A_0$: %s'%(round(f_range,2), round(A_range,1)))\n",
    "\n",
    "plt.title(os.path.splitext(os.path.split(HV_file)[1])[0], fontsize = 14)\n",
    "plt.grid(ls='--')\n",
    "ax.set_yscale('log')\n",
    "plt.xlabel('H/V amplitude', fontsize = 14)\n",
    "plt.ylabel('Frequency ([Hz])', fontsize = 14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f69ae8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We will loop over the HVSR database using pandas groupby to plot each HV profile along the distance. \n",
    "for id, group in db_HVSR.groupby(\"H/V\"):\n",
    "    \n",
    "    # loop over the .hv files\n",
    "    for id,line in group.iterrows():\n",
    "\n",
    "        ID = line.ID + \".hv\"\n",
    "        HV_file = os.path.join(HV_db_folder, 'Analysed', ID)\n",
    "        \n",
    "       # create the plot\n",
    "        fig, ax= plt.subplots(figsize=(10,5))\n",
    "\n",
    "        # get the HV values and plot\n",
    "        Freq, A, A_min, A_max = read_HV(HV_file)\n",
    "        f0_curve, A0_curve, A0_min_curve, A0_max_curve = get_params_from_HV_curve(HV_file) # Get the HV params from the HV plot\n",
    "        plot_HV(Freq, A, A_min, A_max, f0_curve, A0_curve,  A0_min_curve, A0_max_curve, 'manual', 'blue')\n",
    "        \n",
    "        plt.scatter(A0_curve, f0_curve, c = 'red', edgecolors = 'red', alpha = 0.5, zorder = 10, \n",
    "            label='$f_0$: %s Hz \\n$A_0$: %s'%(round(f0_curve,2), round(A0_max_curve,1)))\n",
    "        \n",
    "\n",
    "        plt.title(os.path.splitext(os.path.split(HV_file)[1])[0], fontsize = 14)\n",
    "        plt.grid(ls='--')\n",
    "        ax.set_yscale('log')\n",
    "        plt.xlabel('H/V amplitude', fontsize = 14)\n",
    "        plt.ylabel('Frequency ([Hz])', fontsize = 14)\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d03a3",
   "metadata": {},
   "source": [
    "### Create the HV cross-profile\n",
    "Now, let's make the HVSR profile with distance.\n",
    "\n",
    "__Strategy__: this profile is made by\n",
    "* finding the largest amplitude of all HV curves\n",
    "* normalising all amplitudes to this largest amplitude\n",
    "* converting the amplitude to distance (hack-solution) so it can be plotted \n",
    "\n",
    "__Possibilities__:\n",
    "We can either make a \n",
    "* frequency-distance plot to show the HV curve variation\n",
    "or convert the HV curve to depth by using:\n",
    "* any information someone has on the velocity profile: 6C ping __Felix__ and __Sabrina__?\n",
    "* or using a fixed wild guess velocity\n",
    "* or using a powerlaw equation between f0 and depth in the form of h = _a_ * np.power(f0,_b_), with _a_ and _b_ fixed parameters set to a certain area (e.g. __Heiner__ checking all boreholes in the Bayrischzell valley and doing HVSR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e610ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HV_database = r'HVSR_ex3_Bay\\HVSR_database_Bayrischzell_f0_from_hv.csv'\n",
    "\n",
    "db_HVSR = pd.read_csv(HV_database, encoding='latin')\n",
    "\n",
    "### Choose which profile you want: \n",
    "freq_profile = False ## frequency profile of all HVs\n",
    "\n",
    "depth_profile_fixed = False #depth profile with frequency converted to depth with a fixed velocity \n",
    "Vs = 350 #m/s\n",
    "\n",
    "depth_profile_powerlaw = True #depth profile with frequency converted to depth using a powerlaw equation\n",
    "a_pw = 88.631     # a value of the powerlaw\n",
    "b_pw = -1.683    # b value of the powerlaw\n",
    "\n",
    "### Give the exaggeration value to exaggerate the horizontal scale of the HV plots\n",
    "### e.g. h_exa = 1 will give virtual boreholes\n",
    "h_exa = 80\n",
    "\n",
    "#######################\n",
    "## Main program\n",
    "#######################\n",
    "# create plot\n",
    "fig, ax= plt.subplots(figsize=(16,6))\n",
    "\n",
    "# Create empty arrays to get all the HV data\n",
    "Freqs= []\n",
    "A0s = []\n",
    "Amins = []\n",
    "Amaxs = []\n",
    "f0s_db = []\n",
    "A0s_db = []\n",
    "\n",
    "max_amp = 0\n",
    "\n",
    " \n",
    "# loop over the .hv files\n",
    "for id, line in db_HVSR.iterrows():\n",
    "        ID = line.ID + \".hv\"\n",
    "        HV_file = os.path.join(HV_db_folder, 'Analysed', ID)\n",
    "        \n",
    "        ### load necessary data\n",
    "        Freq, A0, A_min, A_max = read_HV(HV_file)\n",
    "        Freqs.append(Freq)\n",
    "        A0s.append(A0)\n",
    "        Amins.append(A_min)\n",
    "        Amaxs.append(A_max)\n",
    "        # get the maximum amplitude of all HV files to normalise the max amplitude later\n",
    "        if A0.max() > max_amp:\n",
    "            max_amp = A0.max()\n",
    "        f0s_db.append(line.f0_avg)\n",
    "        A0s_db.append(line.A0)\n",
    "        \n",
    "# Normalise the amplitudes and min and max (to potentially plot the error on f0)\n",
    "A0s = pd.Series(A0s)/max_amp\n",
    "Amins = pd.Series(Amins)/max_amp\n",
    "Amaxs = pd.Series(Amaxs)/max_amp\n",
    "\n",
    "# create a pandas db for plotting all curves\n",
    "db_HVs = pd.DataFrame({'Freqs':Freqs, 'A0s':A0s, 'Amins':Amins, 'Amaxs':Amaxs, \n",
    "                       'd_cumul':db_HVSR[\"d_cumul\"], 'Z':db_HVSR[\"Z\"], 'ID':db_HVSR[\"ID\"]})\n",
    "for k, _ in db_HVs.iterrows():        \n",
    "        # convert amplitudes to distances\n",
    "\n",
    "        arrays = []\n",
    "        for ar in [_.A0s, _.Amins, _.Amaxs]:\n",
    "            ar *= h_exa\n",
    "            ar += _.d_cumul\n",
    "            arrays.append(ar)\n",
    "        # if you want a frequency plot y = frequency values        \n",
    "        if freq_profile:\n",
    "            y = _.Freqs\n",
    "            plt.text(_.d_cumul, 0.6, _.ID, rotation = 'vertical', c='gray', zorder = 10)\n",
    "        \n",
    "        # if you want a depth profile with a fixed Vs,  y = Vs / 4*f0            \n",
    "        if depth_profile_fixed:\n",
    "            y = Vs / (_.Freqs * 4)\n",
    "            y= _.Z - y\n",
    "            plt.text(_.d_cumul, 685, _.ID, rotation = 'vertical', c='gray', zorder = 10)\n",
    "    \n",
    "        # if you want a depth profile with a powerlaw conversion,  y = powerlaw            \n",
    "        if depth_profile_powerlaw:\n",
    "            y = a_pw * np.power(_.Freqs,b_pw)\n",
    "            y= _.Z - y\n",
    "        \n",
    "        # color the lines\n",
    "        colorline(arrays[0], y, _.A0s, cmap='viridis', linewidth=5)\n",
    "        colorbar = colorline(arrays[0], y, _.A0s, cmap='viridis', linewidth=5)        \n",
    "        \n",
    "        # scatterplot a dot on f0 and A0\n",
    "        Amax_f0 = np.max(arrays[0])\n",
    "        Fmax_f0 = y[np.argmax(arrays[0])]\n",
    "        plt.scatter(Amax_f0, Fmax_f0, c = 'red', edgecolors = 'red', alpha = 0.5, zorder = 10)\n",
    "        \n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "\n",
    "if freq_profile:\n",
    "        plt.ylabel(\"Frequency [Hz]\",  fontsize=14)\n",
    "        ax.set_yscale('log')\n",
    "        plt.ylim(0.5,100)\n",
    "else:\n",
    "        plt.ylabel(\"Depth [m]\",  fontsize=14)\n",
    "        plt.ylim(680,820)\n",
    "\n",
    "plt.xlabel(\"Distance [m] & Normalized Amplitude\",  fontsize=14)\n",
    "id = '%s '%(db_HVSR[\"Comment\"][0].split('_')[0])\n",
    "plt.title('H/V spectral ratio analysis - %s'%id,  fontsize=16)\n",
    "\n",
    "plt.xlim(0,1000)\n",
    "plt.grid()\n",
    "plt.plot(d_cumul_profile,Z_profile, linewidth=2, c='black')\n",
    "#plt.savefig('Figures/HVSR_profile_Brussels.png', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c68683a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ba430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1a9c376",
   "metadata": {},
   "source": [
    "### What about the variability of each of the H/V curve(s)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf916be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linux\n",
    "geopsy_exe = \"geopsy-hv\"\n",
    "\n",
    "# Windows - locate the Geopsypack.win64-3.4.2 folder\n",
    "geopsy_exe = 'C:/Users/koenvn/geopsypack-win64-3.4.2/bin/geopsy-hv.exe'\n",
    "\n",
    "# For Mac, use a Windows (sorry :-))\n",
    "\n",
    "# test the help file\n",
    "#!{''.join(geopsy_exe)} -help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfbfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall processing lengths (time for which we compute the total HV-curve)\n",
    "#process_len = 3600. # [s] standard 1 hour\n",
    "process_len = 1800. # [s] 0.5 hour\n",
    "\n",
    "# window length for each HV curve\n",
    "time_window_len = 60. # [s] standard 60 or 120s\n",
    "\n",
    "#overlapping windows\n",
    "win_overlap = 50\n",
    "\n",
    "# threshold and percentage\n",
    "threshold = 'RelativeSampleThreshold'\n",
    "threshold_pct = 0.7\n",
    "\n",
    "#Smoothing (in digits)\n",
    "KO = 0.4\n",
    "\n",
    "# lower frequency bound\n",
    "min_freq = 0.5 # [Hz]\n",
    "\n",
    "# upper frequency bound\n",
    "max_freq = 100 # [Hz]\n",
    "\n",
    "# how to calculate horizontals (Squared, Energy, Azimuth, Geometric)\n",
    "horizontals_method = 'Squared' # standard 'Squared'\n",
    "\n",
    "##HORIZONTAL_AZIMUTH is used only when HORIZONTAL_COMPONENTS== 'Azimuth'\n",
    "azimuth = 0\n",
    "\n",
    "# in case you want run the HV rotate module (see exercise 4)\n",
    "want_rotation = False # True, False\n",
    "rotation_steps = 10 # [degree]\n",
    "\n",
    "# Using the Cox et al. 2020 filtering \n",
    "#FREQUENCY_WINDOW_REJECTION_MINIMUM_FREQUENCY\n",
    "rej_min_freq = 0.5\n",
    "#FREQUENCY_WINDOW_REJECTION_MAXIMUM_FREQUENCY\n",
    "rej_max_freq = 50\n",
    "#FREQUENCY_WINDOW_REJECTION_STDDEV_FACTOR\n",
    "rej_stdev = 1.8\n",
    "#FREQUENCY_WINDOW_REJECTION_MAXIMUM_ITERATIONS\n",
    "rej_it = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f9f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give waveforms\n",
    "wf = \"HVSR_ex3_Bay\\\\Raw_Data\\\\453001013.1.2023.02.26.14.40.00.000.*.miniseed\"\n",
    "\n",
    "node_ID = os.path.split(wf)[-1].split('.')[0] # Better to have BE.00617 in the folder name or the full node ID name??\n",
    "print(node_ID)\n",
    "\n",
    "# Give outputfolder where to save the .hv files\n",
    "output_folder = 'HVSR_ex3_Bay/Auto_Analysed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a63ac2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Get the station info\n",
    "st = read(wf)\n",
    "tr = st[0]\n",
    "start = tr.stats.starttime\n",
    "end = tr.stats.endtime\n",
    "station = '%s_%s'%(tr.stats.network, tr.stats.station)\n",
    "print('station: %s'%station)\n",
    "\n",
    "delta = end-start\n",
    "print(\"start: %s\"%start)\n",
    "print(\"end: %s\"%end) \n",
    "print(\"We will get %s .hv files of %ss length out of the stream\"%(int(delta/process_len), process_len))\n",
    "\n",
    "for i in np.arange(start, end, process_len):\n",
    "        time = i\n",
    "        print(time)\n",
    "        tStart = '%s%02d%02d%02d%02d%#05.2f'%(time.year, time.month, time.day, time.hour, time.minute, time.second)\n",
    "        tStart_hv = '%s%02d%02d%02d%02d%#02d'%(time.year, time.month, time.day, time.hour, time.minute, time.second)\n",
    "        endtime = time + process_len\n",
    "        tEnd = '%s%02d%02d%02d%02d%#05.2f'%(endtime.year, endtime.month, endtime.day, endtime.hour, endtime.minute, endtime.second)\n",
    "        \n",
    "        # get the empty auto-paramString\n",
    "        paramsString = get_paramString()\n",
    "        \n",
    "        # adapt an auto-PARAM file with the given parameters so that for each processing loop the same param is used\n",
    "        with open('geopsy-hv-auto.params', 'w') as f:\n",
    "            f.write(paramsString.format(\n",
    "                # Select start and end time from waveform\n",
    "                tStart=tStart,\n",
    "                # Select end time from waveform\n",
    "                tEnd=tEnd,\n",
    "                # Adapt the parameters of previously chosen parameters \n",
    "                threshold=threshold,\n",
    "                threshold_pct=threshold_pct,\n",
    "                KO=KO,\n",
    "                winLen=time_window_len,\n",
    "                overlap=win_overlap,\n",
    "                minFreq=min_freq,\n",
    "                maxFreq=max_freq,\n",
    "                horizontals=horizontals_method,\n",
    "                azimuth=azimuth,\n",
    "                rotSteps=rotation_steps,\n",
    "                rej_min_freq=rej_min_freq, \n",
    "                rej_max_freq=rej_max_freq, \n",
    "                rej_stdev=rej_stdev,\n",
    "                rej_it = rej_it\n",
    "                ))\n",
    "        \n",
    "        ### Make a folder for each station output\n",
    "        os.makedirs(output_folder+node_ID, exist_ok=True)\n",
    "        out_folder = ''.join(output_folder+node_ID)\n",
    "        \n",
    "        ### Run geopsy for each step in the loop\n",
    "        !{''.join(geopsy_exe)} -hv {''.join(wf)} -param geopsy-hv-auto.params -o {''.join(output_folder+node_ID)}\n",
    "        \n",
    "        ### Rename the .hv output file and the .log to save a unique files for each processed process_len\n",
    "        ### saving the .log files is useful as these can be loaded in Geopsy to manually check the processed data \n",
    "        os.renames(os.path.join(output_folder, '{0}/{1}.hv'.format(node_ID, station)), os.path.join(output_folder, '{0}/{1}.{2}.hv'.format(node_ID, station, tStart_hv)))\n",
    "        os.renames(os.path.join(output_folder, '{0}/{1}.log'.format(node_ID, station)), os.path.join(output_folder, '{0}/{1}.{2}.log'.format(node_ID, station, tStart_hv)))\n",
    "\n",
    "        # if want_rotation is selected, also the HV rotate module will be executed and stored in the outfolder  \n",
    "        if want_rotation:\n",
    "            \n",
    "            # run the geopsy rotation\n",
    "            !{''.join(geopsy_exe)} -rotate {''.join(wf)} -param geopsy-hv-auto.params -o {''.join(output_folder+node_ID)}\n",
    "            \n",
    "            ### Rename the .hv.grid output file\n",
    "            os.renames(os.path.join(output_folder, '{0}/{1}.hv'.format(node_ID, station)), os.path.join(output_folder, '{0}/{1}.{2}.grid'.format(node_ID, station, tStart_hv)))\n",
    "            \n",
    "        print('**********************************')\n",
    "\n",
    "print('Job Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e1180",
   "metadata": {},
   "source": [
    "## Visualise Bayrischzell auto_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37657a88",
   "metadata": {},
   "source": [
    "### HV variability of the node with time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165712b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# give the output folder containing all subfolders with continuous hv files\n",
    "output_folder = 'HVSR_ex3_Bay\\Auto_Analysed'\n",
    "\n",
    "###################################################################\"\"\n",
    "# create the continuous HV plot\n",
    "fig, ax1= plt.subplots(figsize=(10,5))\n",
    "\n",
    "# loop over the subfolders\n",
    "folders = glob.glob(\"%s/453001013/\"%output_folder, recursive = True)\n",
    "print(folders)\n",
    "\n",
    "for i in folders:\n",
    "\n",
    "    HV_files = glob.glob('%s/*.hv'%i)\n",
    "    \n",
    "    # loop over the HV files\n",
    "    for i,j in zip(HV_files, np.arange(0,len(HV_files),1)):\n",
    "        node_ID = os.path.split(i)[-1].split('.')[0]\n",
    "        # read the time from the HV name\n",
    "        time = os.path.split(i)[1].split('.')[1]\n",
    "        year = time[0:4]\n",
    "        d = datetime.datetime(int(time[0:4]),int(time[4:6]), int(time[6:8]), int(time[8:10]), int(time[10:12]), int(time[12:14]))\n",
    "        print(i, d)\n",
    "    \n",
    "        Freq, A, A_min, A_max = read_HV(i)\n",
    "        plt.plot(A+j, Freq, c='gray', alpha=0.6)\n",
    "        \n",
    "        f0_curve, A0_curve, A0_min_curve, A0_max_curve = get_params_from_HV_curve(i)\n",
    "        plt.scatter(A0_curve+j, f0_curve, c = 'red', edgecolors = 'red', alpha = 0.5, zorder = 10, \n",
    "            label='$f_0$: %s Hz \\n$A_0$: %s'%(round(f0_curve,2), round(A0_max_curve,1)))\n",
    "        \n",
    "\n",
    "    plt.ylim(2,50)\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xlabel('Some meaningless value (time)', fontsize = 14)\n",
    "    ax1.set_ylabel('Frequency [Hz]', fontsize = 14)\n",
    "    ax1.grid(ls='--', axis='y', which='both')\n",
    "    plt.subplots_adjust(hspace=0.1)\n",
    "    #ax1.tick_params(labelbottom=True)\n",
    "    plt.title('HVSR variability through time', fontsize = 14)\n",
    "\n",
    "    plt.xticks(rotation = 90)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d8571",
   "metadata": {},
   "source": [
    "### HV variability of all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74204ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# give the output folder containing all subfolders with continuous hv files\n",
    "output_folder = 'HVSR_ex3_Bay\\Auto_Analysed'\n",
    "\n",
    "###################################################################\"\"\n",
    "# create the continuous HV plot\n",
    "fig, ax1= plt.subplots(figsize=(10,5))\n",
    "\n",
    "# loop over the subfolders\n",
    "folders = glob.glob(\"%s/*/\"%output_folder, recursive = True)\n",
    "print(folders)\n",
    "\n",
    "for i in folders:\n",
    "    \n",
    "    f0_wins = []\n",
    "    f0_avgs = []\n",
    "    A0s = []\n",
    "    times = []\n",
    "    errors_min, errors_max = [], []\n",
    "    HV_files = glob.glob('%s/*.hv'%i)\n",
    "    \n",
    "    # loop over the HV files\n",
    "    for i in HV_files:\n",
    "        node_ID = os.path.split(i)[-1].split('.')[0]\n",
    "        # read the time from the HV name\n",
    "        time = os.path.split(i)[1].split('.')[1]\n",
    "        year = time[0:4]\n",
    "        d = datetime.datetime(int(time[0:4]),int(time[4:6]), int(time[6:8]), int(time[8:10]), int(time[10:12]), int(time[12:14]))\n",
    "        print(i, d)\n",
    "\n",
    "        f0_avg, f0_win, error, A0, nw_avg, nw_win, f_min, f_max = get_params_from_HV(i)\n",
    "\n",
    "        f0_avgs.append(f0_avg)\n",
    "        A0s.append(A0)\n",
    "        f0_wins.append(f0_win)\n",
    "        errors_min.append(f0_avg-f_min)\n",
    "        errors_max.append(f_max-f0_avg)\n",
    "        times.append(d)\n",
    "\n",
    "    plt.scatter(times, f0_avgs, ls='-', label = node_ID)    \n",
    "    plt.errorbar(times, f0_avgs, xerr=None, yerr = (errors_min, errors_max), c='grey',  alpha=0.5, zorder=-1, ls='none')   \n",
    "    ax1.set_yscale('log')\n",
    "    ax1.yaxis.set_minor_formatter(mticker.ScalarFormatter())\n",
    "    ax1.yaxis.set_major_formatter(mticker.ScalarFormatter())\n",
    "    ax1.set_xlabel('Datetime', fontsize = 14)\n",
    "    ax1.set_ylabel('Frequency [Hz]', fontsize = 14)\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 0), loc=\"lower left\", borderaxespad=0, ncol=1)\n",
    "    ax1.grid(ls='--', axis='y', which='both')\n",
    "    plt.subplots_adjust(hspace=0.1)\n",
    "    ax1.tick_params(labelbottom=True)\n",
    "    plt.title('HVSR variability', fontsize = 14)\n",
    "\n",
    "    #Format the xaxis date\n",
    "    from matplotlib.dates import DateFormatter, DayLocator, HourLocator\n",
    "\n",
    "    ax1.xaxis.set_major_locator(HourLocator())\n",
    "    ax1.xaxis.set_major_formatter(DateFormatter(\"%H:%M\"))\n",
    "    plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702fead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
